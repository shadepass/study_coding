# 머신러닝 기본 개념
## 머신러닝 프로세스
데이터 수집 → 데이터 정리(테스트데이터셋) → 모델 학습 → 모델테스트(성능이 좋지 않다면 다시 모델 학습) → 모델 배포

## 머신러닝 기본 개념
과거: 데이터, 모델, prediction  
사람이 직접 데이터 패턴을 찾아 알고리즘 코딩하여 결과

→ 자동화 →

AI/머신러닝: 데이터, 모델, Prediction  
데이터와 결과를 기반으로 스스로 패턴 학습하고 이를 이용해서 예측  
지금은 모델을 만들고 데이터와 결과만 주면 됨.

## Linear Regrssion
직선 회귀(다시 돌아봄), 직선을 그어서 예측하는 모델

가설: 공부를 많이 하면 공부를 잘할 것이다.  
scatter 그래프 상 우상향 보임

점들을 잘 표현하는 한 직선을 구하는게 목표  
→ 그 의미는 가중치와 절편을 구한다는 의미

y = wx + b, w 가중치, b 절편, x 시간, y 점수 → 모델

컴퓨터는 어떤 가중치가 최적인지 어떻게 알 수 있는가  
> cost Function 이용!(작은 Cost로 결정)

Cost Function = (실제값 - 에측값)^2 / N  
→ MSE(Mean Squared Error)

## Gradient Descent Algorithm(2차원 함수)
Cost Function 최적화

제곱함수는 포물선 그림이 그려짐

그래프상 비용이 제일 낮은 아래부분(기울기 0) 을 찾으면 된다.
![Gradient Descent Algorithm](image.png)

![alt text](image-1.png)

## 모델학습이란
목표: 최적의 직선 구하기→  
직선별 손실함수 구하기→  
손실함수 최소값 구하기→  
Gradient Descent Algorithm

# 머신러닝 기술 원리(종류, 성능평가 등)
## 데이터 확보
잘 정리된 데이터 확보가 중요

## 지도학습 vs 비지도학습
레이블(정답)이 있으냐 없는가가 가장 큰 차이

### 지도학습
분류모델, 예측모델
분류모델: 레이블의 값들이 이산적으로 나누어질 수 있는 문제
예측모델: 연속적인 값

데이터셋 구조
각 열을 특징/속성(featre) + 각 행을 예제 데이터라고 함

컬럼 중 하나를 선택해서 레이블(정답)으로 사용

#### 데이터 셋 분리
Train / Test or  
Train or  Validation or Test

#### 모델 선택
좋은 모델이란, 데이터의 패턴을 정확하게 따라가는 모델

과적합(Overfitting): 숨어있는 데이터에 대해서 못 맞추는 문제 발생  
 - 학습데이터만으로 했기 때문에 숨어있는건 못 맞춤

과소적합(Underfitting): 학습 많이 해야함

모델성능평가: Training과 Test가 비슷하게 가야함

#### 성능지표
학습이 끝난 후 모델을 성능평가하는 용도
- 회귀모델: MSE, MAE, R2
- 분류모델: 정확도, 정밀도, 재현율, F1-점수

회귀 성능지표  
1. MSE(Mean Squared Error): 예측값에 대한 실제값의 오차를 구하고 그 제곱값의 평균을 구하는 방식

2. MAE(Mean Absoulte Error): 예측값에 대한 실제값의 오차를 구하고 그 절댓값의 평균을 구하는 방식

3. R2, 결정계수: 회귀 모델에서 독립변수가 종속변수를 얼마나 잘 설명해주는지  
1에 가까울 수록 좋은 모델

분류모델 성능지표: Confusion Matrix(오차행렬)을 기반으로 수치 표현  
T(맞느냐 틀렸느냐)N(예측한값) → TN, FP, FN, TP

1. 정확도 = TP + Tn / TP + FN + FP + TN  
정확도의 함정 존재

2. 정밀도 = TP / TP + FP  
모델이 True라고 분류한 것 중에서 실제 True인 분류

3. 재현율: TP / TP + FN  
실제 True인 것중에서 모델이 True라고 예측한 것

4. F1점수(F1-score): 정밀도와 재현율의 조화평균  
= 2 * 1/(1/Precision + 1/Recall)

# 머신러닝 주요 알고리즘
## Scikit-learn
가장 인기 있는 머신러닝 패키지, 많은 머신러닝 알고리즘이 내장되어 있음

```python
from sklearn.family import Model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
```

## 주요 알고리즘 분류
회귀: Linear Regression  
분류: Logistic Regrssion  
회귀+분류: Desicion Tree, Random Forest, K-Nearest Neighbor

## Linear Regression
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
pred = model.predict(X_Test)
```

## Logistic Regression
이진 분류 규칙은 0과 1의 두 클래스를 갖는것으로,  
일반 선형 회귀 모델을 이진분류에 사용하기 어려움

점들을 잘 표현하는 직선 그리기가 어려움

로지스틱 함수를 사용하여 결과값을 0~1사이의 값으로 변환하여 이진 분류 할 수 있음

```py
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
pred = model.predict(X_test)
```

## K-Nearest Neighbor
새로운 데이터가 주어졌을 때, 기존 데이터 가운데 가장 가까운 K개이 이웃의 정보로 새로운 데이터 유추

```py
from sklearn.neightbors import KNeightborsClassifier
knn = KNeighborsClossifier(n_neighbors = 3) # 3은 하이퍼파라미터, 모델에게 직접 주어지는 정보
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
```

## Decision Tree
분류와 회귀 작업이 가능한 다재다능한 머신러닝 알고리즘 

복잡한 데이터셋도 학습할 수 있으며, 강력한 머신러닝 알고리즘인 랜덤 포레스트의 기본 구성 요소

머신러닝이 왜 그렇게 분류했는지 설명할 수 있어서 예전에 많이 사용됨

```py
from sklearn.tree import DecisionTreeClassifier
model=DeicisionTreeClassifer(max_depth = 2)
model.fit(X_train, Y_train)
pred = model.predict(X_test)
```

## Random Forest
일련의 예측기(분류, 회귀모델)로 부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있음

앙상블 계열의 모델

훈련 세트로부터 무작위로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련시킬 수 있음

```py
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 50)
model.fit(X_train, y_train)
pred = model.predict(X_test)
```

### 랜덤포레스트 하이퍼 파라미터
n_estimators = 생성할 트리의 개수(int, default = 100)  
max_depth = 트리의 최대 깊이(int, default = None)  
min_sampels_split= 노드를 분할하기 위해 필요한 최소한의 샘플 개수(int or float, default = 2)

### 변수중요도
feature_importances_ = 변수에 대한 중요도 값을 제공

## GridSearchCV
하이퍼파라미터를 순차적으로 입력해 학습을 하고 측정을 하면서 가장 최적의 파라미터를 알려줌

```py
from sklearn.model_selection import GridSearchCV
rfc = RandomForestClassifier()
params {'n_estimators': [100, 150], 'max_depth': [2, 5]}
grid_rfc = GridSearchCV(rfc, param_grid = params)
grid_rfc.fit(X_train, y_train)
```

### 중요속성
grid_Rfc.fit(X_train, y_Train)
grid_rfc.best_params: 최적의 파라미터 리스트
gird_rfc.best_score_: 최적의 파라미터일때의 score

